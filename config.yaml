model:
  name: null  # facebook/bart-base / bert-base-uncased
  mode: s2s
  pretrain: null
  pretrain_encoder: null
  use_teacher: False
  teacher_weight: 0.8 # 0.8QQP
  teacher_lambda: 1
  teacher: null
  tokenizer: null
  context_aware: null # use bert output to guild training
  logits_aware: null # use bert logits output to guild training
  time_weighted: False
  aux_ar_model: False
  aux_ar_model_layers: 2
  aux_ar_weight: 6  # QQP
  aux_ar_lambda: 1 
  reconstruct_tgt: x

use_reg_l2: False
use_reg_random: False
use_decay_reg_weight: False
reg_weight_scale: 2

exp:
  seed: 101
  root: ./my_output
  name: test
  dir: null

visible_device: 0
device: cuda

data:
  path: ./data
  name: commongen

time_channels: 64
in_channels: 64
out_channels: 64
diffusion_steps: 2000

tgt_len: 54  # iwslt14->100 / commengen->54 / cnndm->? / xsum->150
max_pos_len: 256  # iwslt14->256 / commengen->128 / cnndm->? / xsum->512

src_lang: de
tgt_lang: en

intermediate_size: 1024
num_attention_heads: 4

fairseq:
  use_fairseq: False
  real_data: False
  dist_data: False

vocab_size: 10000

use_mbert: False  # mt 
use_bpe: False  # mt 
pad_value: 0  # mt
num_workers: 4  # bpe->4

# training params
lr_step: 40000
warmup_steps: 16000
total_steps: 240000
batch_size: 384
lr: 2e-4
weight_decay: 0.0
grad_clip: -1.0
ema_rate: 0.9999
grad_accum: 1

eval_interval: 2000
log_interval: 500
save_interval: 10000

#*************************************
# something can change
clip_scale: 0.0
use_step_ratio: False
ratio_thre: 0.7
label_smooth: 0.0
scale_embedding: False

continue_train: False
use_AMP: False
grad_penalty: False
loss_aware: False
pred_len: False
length_factor: 0.1
init_weight: False
prediction: False
pred_len_strategy: null  # token_embed / mean_pool

att_strategy: null  # txl / rotary / null
rel_postion: False
position_att: False
time_att: False
infer_self_condition: False
self_condition: False
schedule_sampler: uniform  # uniform / xy_uniform / xy3_uniform / loss-second-moment
end_point_scale: 2.0
dropout: 0.1
att_dropout: 0.1

num_samples: 1
ddim_sample: False
skip_timestep: 100

skip_sample: False
gen_timesteps: 20
#*************************************

# These parameters remain unchanged for now
predict_xstart: True
rescale_timesteps: True
resume_checkpoint: True

shared_embeds: False
roformer_timeAtt: False
add_layer_time: False
use_sentence_piece: False
load_encoder: False
predict_x_start: False
load_bart: False
use_kl: False
fix_encoder: False
learn_pos: False

sigma_small: False
learn_sigma: False
rescale_learned_sigmas: False

logits_mode: 1  # 1 / 2
noise_schedule: sqrt
emb_type: random  # pretrain / random

# generate params
clip_denoised: False
load_from_ema: False
load_step: 0

# pretrain params
mask_pro: 0.3
pre_max_len: 512

add_retrieval_sentences: False
retrieval_top_k: 1

use_stanza: False

# dp params
dp_head_layer: 1  # 0
use_src_dp: False

# 
control_sample: False